


#Attachments area

from keras.models import Sequential
from keras.layers.core import Flatten, Dense, Dropout, Lambda
from keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D
from keras.optimizers import SGD
import cv2, numpy as np
#!pip install tensorflow_gpu==1.14.0
from keras import backend as K
from keras.datasets import mnist,cifar10
from keras.preprocessing.image import img_to_array, array_to_img, ImageDataGenerator
from keras.utils import to_categorical
import matplotlib.pyplot as plt


subtract_pixel_mean = True
#!pip install
#K.set_image_dim_ordering('tf')
# (train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# train_images = np.reshape(train_images,(train_images.shape[0], 28, 28,1))
# test_images = np.reshape(test_images,(test_images.shape[0],28, 28,1))

(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()

data_augmentation = True

train_images = np.asarray([img_to_array(array_to_img(im, scale=False).resize((224,224))) for im in train_images])

test_images = np.asarray([img_to_array(array_to_img(im, scale=False).resize((224,224))) for im in test_images])



print(train_images.shape)


train_images = train_images/ 255.0
test_images = test_images / 255.0
if subtract_pixel_mean:
    x_train_mean = np.mean(train_images, axis=0)
    train_images -= x_train_mean
    test_images -= x_train_mean
train_labels = to_categorical(train_labels,10)
test_labels = to_categorical(test_labels,10)

model = Sequential()
#model.add(Lambda(vgg_preprocess, input_shape=(224,224,3), output_shape=(224,224,3)))
model.add(ZeroPadding2D((1,1),input_shape=(224,224,3)))
model.add(Convolution2D(64, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(64, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))

model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(128, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(128, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))#, dim_ordering="th"))

model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(256, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))

model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))

model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(ZeroPadding2D((1,1)))
model.add(Convolution2D(512, 3, 3, activation='relu'))
model.add(MaxPooling2D((2,2), strides=(2,2)))

model.add(Flatten())
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(4096, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(10, activation='softmax'))

model.summary()
sgd = SGD(lr=0.001, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(optimizer=sgd,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

#history = model.fit(train_images, train_labels, batch_size=32,epochs=50, validation_data=(test_images,test_labels))

if not data_augmentation:
	
	history = model.fit(train_images, train_labels, batch_size=32,epochs=50, validation_data=(test_images,test_labels))
else:
    print('Using real-time data augmentation.')
    # This will do preprocessing and realtime data augmentation:
    datagen = ImageDataGenerator(
        rotation_range=20,  # randomly rotate images in the range (degrees, 0 to 180)
        # randomly shift images horizontally (fraction of total width)
        width_shift_range=0.2,
        # randomly shift images vertically (fraction of total height)
        height_shift_range=0.2,
        fill_mode='nearest',
        cval=0.,  # value used for fill_mode = "constant"
        horizontal_flip=True,  # randomly flip images
        vertical_flip=False,  # randomly flip images
        validation_split=0.0)

   
    datagen.fit(train_images)

    # Fit the model on the batches generated by datagen.flow().
    history = model.fit_generator(datagen.flow(train_images, train_labels,
                                     batch_size= 32),steps_per_epoch=len(train_images)/32,
                        epochs=50,
                        validation_data=(test_images, test_labels),
                        workers=4)


Results = model.evaluate(test_images,  test_labels, verbose=1)

print('\nTest Loss :', Results[0],'\nTest accuracy:', Results[1]*100)

# loss and accuracy plot

acc = history.history['acc']
val_acc = history.history['val_acc']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)


plt.title('Training and validation accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.plot(epochs, acc, 'red', label='Training acc')
plt.plot(epochs, val_acc, 'blue', label='Validation acc')


plt.legend()


plt.figure()
plt.title('Training and validation loss for')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.plot(epochs, loss, 'red', label='Training loss')
plt.plot(epochs, val_loss, 'blue', label='Validation loss')
